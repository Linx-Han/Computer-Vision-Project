# model.py
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from tqdm import tqdm
import os
from data import get_dataloaders

# ============= æ¨¡å‹å®šä¹‰ =============
class CalorieEstimatorCNN(nn.Module):
    """åŒæµCNNï¼šåˆ†åˆ«å¤„ç†RGBå’ŒDepthï¼Œç„¶åèåˆ"""
    def __init__(self):
        super(CalorieEstimatorCNN, self).__init__()
        
        # RGBæµ
        self.rgb_stream = nn.Sequential(
            # Conv1
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Conv2
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Conv3
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Conv4
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # Depthæµ
        self.depth_stream = nn.Sequential(
            # Conv1
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Conv2
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Conv3
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            # Conv4
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        
        # èåˆå±‚ + å›å½’å¤´
        self.fusion = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1)
        )
    
    def forward(self, rgb, depth):
        # æå–ç‰¹å¾
        rgb_feat = self.rgb_stream(rgb).flatten(1)      # [batch, 256]
        depth_feat = self.depth_stream(depth).flatten(1) # [batch, 256]
        
        # èåˆ
        fused = torch.cat([rgb_feat, depth_feat], dim=1) # [batch, 512]
        
        # å›å½’
        calories = self.fusion(fused).squeeze(1)         # [batch]
        
        return calories


# ============= è®­ç»ƒå‡½æ•° =============
def train_epoch(model, train_loader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    
    for rgb, depth, calories in tqdm(train_loader, desc='Training'):
        rgb = rgb.to(device)
        depth = depth.to(device)
        calories = calories.to(device)
        
        # å‰å‘ä¼ æ’­
        pred_calories = model(rgb, depth)
        loss = criterion(pred_calories, calories)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    avg_loss = total_loss / len(train_loader)
    return avg_loss


def validate(model, val_loader, criterion, device):
    """éªŒè¯"""
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for rgb, depth, calories in tqdm(val_loader, desc='Validation'):
            rgb = rgb.to(device)
            depth = depth.to(device)
            calories = calories.to(device)
            
            pred_calories = model(rgb, depth)
            loss = criterion(pred_calories, calories)
            
            total_loss += loss.item()
    
    avg_loss = total_loss / len(val_loader)
    return avg_loss


# ============= ä¸»è®­ç»ƒæµç¨‹ =============
def main():
    # è¶…å‚æ•°
    BATCH_SIZE = 16
    EPOCHS = 50
    LEARNING_RATE = 0.001
    VAL_SPLIT = 0.2
    
    # ä»ç¯å¢ƒå˜é‡è¯»å–è·¯å¾„
    ROOT_DIR = os.getenv('DATA_ROOT_DIR', './data')
    CSV_FILE = os.getenv('TRAIN_CSV_FILE', './data/nutrition5k_train.csv')
    CHECKPOINT_DIR = os.getenv('CHECKPOINT_DIR', './checkpoints')
    
    print(f"ğŸ“ æ•°æ®æ ¹ç›®å½•: {ROOT_DIR}")
    print(f"ğŸ“„ è®­ç»ƒCSVæ–‡ä»¶: {CSV_FILE}")
    print(f"ğŸ’¾ æ£€æŸ¥ç‚¹ç›®å½•: {CHECKPOINT_DIR}")
    
    # è®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device("mps")   # Apple Silicon GPU
        print("âœ… ä½¿ç”¨ Apple Silicon GPU (MPS)")
    else:
        device = torch.device("cpu")
        print("âš ï¸ ä½¿ç”¨ CPU")
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    
    # æ•°æ®åŠ è½½
    print("\nåŠ è½½æ•°æ®...")
    train_loader, val_loader = get_dataloaders(
        root_dir=ROOT_DIR,
        csv_file=CSV_FILE,
        batch_size=BATCH_SIZE,
        val_split=VAL_SPLIT
    )
    print(f"è®­ç»ƒé›†: {len(train_loader.dataset)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_loader.dataset)} æ ·æœ¬")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºæ¨¡å‹...")
    model = CalorieEstimatorCNN().to(device)
    
    # æ‰“å°æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )
    
    # è®°å½•è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_rmse': []
    }
    
    # è®­ç»ƒ
    print("\nå¼€å§‹è®­ç»ƒ...\n")
    best_val_loss = float('inf')
    
    for epoch in range(EPOCHS):
        print(f"Epoch {epoch+1}/{EPOCHS}")
        print("-" * 50)
        
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        
        # éªŒè¯
        val_loss = validate(model, val_loader, criterion, device)
        val_rmse = np.sqrt(val_loss)
        
        # è®°å½•å½“å‰å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']
        
        # è°ƒæ•´å­¦ä¹ ç‡
        old_lr = current_lr
        scheduler.step(val_loss)
        new_lr = optimizer.param_groups[0]['lr']
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_rmse'].append(val_rmse)
        
        print(f"Train Loss: {train_loss:.4f}")
        print(f"Val Loss: {val_loss:.4f}")
        print(f"Val RMSE: {val_rmse:.4f}")
        print(f"Learning Rate: {new_lr:.6f}")
        
        # å¦‚æœå­¦ä¹ ç‡æ”¹å˜äº†ï¼Œæ‰“å°æç¤º
        if new_lr < old_lr:
            print(f"âš¡ å­¦ä¹ ç‡é™ä½: {old_lr:.6f} -> {new_lr:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            checkpoint_path = os.path.join(CHECKPOINT_DIR, 'best_model.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_rmse': val_rmse,
            }, checkpoint_path)
            print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {val_loss:.4f}, RMSE: {val_rmse:.4f})")
        
        print()
    
    # ä¿å­˜è®­ç»ƒå†å²
    history_path = os.path.join(CHECKPOINT_DIR, 'training_history.npy')
    np.save(history_path, history)
    
    print("\nè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯Loss: {best_val_loss:.4f}")
    print(f"æœ€ä½³éªŒè¯RMSE: {np.sqrt(best_val_loss):.4f}")
    
    # æ‰“å°æœ€åå‡ ä¸ªepochçš„ç»“æœ
    print("\næœ€å5ä¸ªepoch:")
    for i in range(max(0, len(history['val_rmse'])-5), len(history['val_rmse'])):
        print(f"  Epoch {i+1}: Train Loss={history['train_loss'][i]:.4f}, "
              f"Val Loss={history['val_loss'][i]:.4f}, "
              f"Val RMSE={history['val_rmse'][i]:.4f}")


if __name__ == '__main__':
    main()